{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demanding-antibody",
   "metadata": {},
   "source": [
    "# Deploying Punctuation and Capitalization Model in RIVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-quarterly",
   "metadata": {},
   "source": [
    "[Train Adapt Optimize (TAO) Toolkit ](https://developer.nvidia.com/tao-toolkit) provides the capability to export your model in a format that can deployed using NVIDIA [Riva](https://developer.nvidia.com/riva), a highly performant application framework for multi-modal conversational AI services using GPUs.\n",
    "\n",
    "This tutorial explores taking a .riva model, the result of `tao punctuation_and_capitalization export` command, and leveraging the Riva ServiceMaker framework to aggregate all the necessary artifacts for Riva deployment to a target environment. Once the model is deployed in Riva, you can issue inference requests to the server. We will demonstrate how quick and straightforward this whole process is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-rider",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn how to:  \n",
    "- Use Riva ServiceMaker to take a TAO exported .riva and convert it to .rmir\n",
    "- Deploy the model(s) locally  on the Riva Server\n",
    "- Send inference requests from a demo client using Riva API bindings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-trance",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Before going through the jupyter notebook, please make sure:\n",
    "- You have access to NVIDIA NGC, and are able to download the Riva Quickstart [resources](https://ngc.nvidia.com/catalog/resources/nvidia:riva:riva_quickstart)\n",
    "- Have a .riva model file that you wish to deploy. You can obtain this from ``tao <task> export`` (with ``export_format=RIVA``). \n",
    "\n",
    "<b>NOTE:</b> Please refer to the tutorial on *Punctuation And Capitalization using Train Adapt Optimize (TAO) Toolkit* for more details on training and exporting a .riva model for punctuation and capitalization task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-schedule",
   "metadata": {},
   "source": [
    "## Riva ServiceMaker\n",
    "\n",
    "Servicemaker is the set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Riva deployment to a target environment. It has two main components as shown below:\n",
    "\n",
    "### 1. Riva-build\n",
    "\n",
    "This step helps build a Riva-ready version of the model. Itâ€™s only output is an intermediate format (called a RMIR) of an end to end pipeline for the supported services within Riva. We are taking a ASR QuartzNet Model in consideration<br>\n",
    "\n",
    "`riva-build` is responsible for the combination of one or more exported models (.riva files) into a single file containing an intermediate format called Riva Model Intermediate Representation (.rmir). This file contains a deployment-agnostic specification of the whole end-to-end pipeline along with all the assets required for the final deployment and inference. Please checkout the [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/service-nlp.html#pipeline-configuration) to find out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Set the following variables\n",
    "\n",
    "# ServiceMaker Docker\n",
    "RIVA_SM_CONTAINER = \"<Riva_Servicemaker_Image>\"\n",
    "\n",
    "# Directory where the .riva model is stored $MODEL_LOC/*.riva\n",
    "MODEL_LOC = \"<path_to_model_directory>\"\n",
    "\n",
    "# Name of the .riva file\n",
    "MODEL_NAME = \"<add model name>\"\n",
    "\n",
    "# Use the same key that .riva model is encrypted with\n",
    "KEY = \"<add encryption key used for trained model>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the ServiceMaker Image\n",
    "!docker pull $RIVA_SM_CONTAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax: riva-build <task-name> output-dir-for-rmir/model.rmir:key dir-for-riva/model.riva:key\n",
    "!docker run --rm --gpus all -v $MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "    riva-build punctuation -f /data/punct-capit.rmir:$KEY /data/$MODEL_NAME:$KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-flash",
   "metadata": {},
   "source": [
    "`NOTE:` Above, punct-capit-model.rmir is the punctuation and capitalization model obtained from `tao punctuation_and_capitalization export`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-kuwait",
   "metadata": {},
   "source": [
    "### 2. Riva-deploy\n",
    "\n",
    "The deployment tool takes as input one or more Riva Model Intermediate Representation (RMIR) files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for the execution and finally writes all those assets to the output model repository directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir:key output-dir-for-repository\n",
    "!docker run --rm --gpus all -v $MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "     riva-deploy -f /data/punct-capit.rmir:$KEY /data/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-chapel",
   "metadata": {},
   "source": [
    "## Start Riva Server\n",
    "\n",
    "Once the model repository is generated, we are ready to start the Riva server. From this step onwards you need to download the Riva QuickStart Resource from NGC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the path to Riva directory\n",
    "RIVA_DIR = <path_to_riva_quickstart>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-table",
   "metadata": {},
   "source": [
    "Next, we modify ``config.sh`` to enable relevant Riva services (nlp for Punctuation & Capitalization model), provide the encryption key, and path to the model repository (``riva_model_loc``) generated in the previous step among other configurations.\n",
    "\n",
    "Pretrained versions of models specified in models_asr/nlp/tts are fetched from NGC. Since we are using our custom model, we can comment it in models_nlp (and any others that are not relevant to our use case). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99849e4-e7c7-4320-abbc-1293f1c86bfa",
   "metadata": {},
   "source": [
    "#### config.sh snipet\n",
    "```\n",
    "# Enable or Disable Riva Services \n",
    "service_enabled_asr=false                                 ## MAKE CHANGES HERE\n",
    "service_enabled_nlp=true                                  ## MAKE CHANGES HERE\n",
    "service_enabled_tts=false                                 ## MAKE CHANGES HERE\n",
    "\n",
    "# Specify one or more GPUs to use\n",
    "# specifying more than one GPU is currently an experimental feature, and may result in undefined behaviours.\n",
    "gpus_to_use=\"device=0\"\n",
    "\n",
    "# Specify the encryption key to use to deploy models\n",
    "MODEL_DEPLOY_KEY=\"tlt_encode\"                             ## Set the model encryption key\n",
    "\n",
    "# Locations to use for storing models artifacts\n",
    "...\n",
    "riva_model_loc=\"<add path>\"                              ## Replace with MODEL_LOC\n",
    "\n",
    "# The default RMIRs are downloaded from NGC by default in the above $riva_rmir_loc directory\n",
    "# If you'd like to skip the download from NGC and use the existing RMIRs in the $riva_rmir_loc\n",
    "# then set the below $use_existing_rmirs flag to true.\n",
    "...\n",
    "use_existing_rmirs=true                                  ## Set to True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts.\n",
    "!cd $RIVA_DIR && chmod +x ./riva_init.sh && chmod +x ./riva_start.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Riva Init. This will fetch the containers/models\n",
    "# YOU CAN SKIP THIS STEP IF YOU DID RIVA DEPLOY\n",
    "!cd $RIVA_DIR && ./riva_init.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Run Riva Start. This will deploy your model(s).\n",
    "!cd $RIVA_DIR && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-greensboro",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "Once the Riva server is up and running with your models, you can send inference requests querying the server. \n",
    "\n",
    "To send GRPC requests, you can install Riva Python API bindings for client. This is available as a pip .whl with the QuickStart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Set the name of the whl file\n",
    "RIVA_API_WHL = \"<add riva api .whl file name>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install client API bindings\n",
    "!cd $RIVA_DIR && pip install $RIVA_API_WHL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-sense",
   "metadata": {},
   "source": [
    "Run the following sample code from within the client docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import argparse\n",
    "import os\n",
    "import riva_api.riva_nlp_pb2 as rnlp\n",
    "import riva_api.riva_nlp_pb2_grpc as rnlp_srv\n",
    "\n",
    "class BertPunctuatorClient(object):\n",
    "    def __init__(self, grpc_server, model_name=\"riva_punctuation\"):\n",
    "        # generate the correct model based on precision and whether or not ensemble is used\n",
    "        print(\"Using model: {}\".format(model_name))\n",
    "        self.model_name = model_name\n",
    "        self.channel = grpc.insecure_channel(grpc_server)\n",
    "        self.riva_nlp = rnlp_srv.RivaLanguageUnderstandingStub(self.channel)\n",
    "\n",
    "        self.has_bos = True\n",
    "        self.has_eos = False\n",
    "\n",
    "    def run(self, input_strings):\n",
    "        if isinstance(input_strings, str):\n",
    "            # user probably passed a single string instead of a list/iterable\n",
    "            input_strings = [input_strings]\n",
    "\n",
    "        request = rnlp.TextTransformRequest()\n",
    "        request.model.model_name = self.model_name\n",
    "        for q in input_strings:\n",
    "            request.text.append(q)\n",
    "        response = self.riva_nlp.TransformText(request)\n",
    "\n",
    "        return response.text[0]\n",
    "\n",
    "def run_punct_capit(server,model,query):\n",
    "    print(\"Client app to test punctuation and capitalization on Riva\")\n",
    "    client = BertPunctuatorClient(server, model_name=model)\n",
    "    result = client.run(query)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_punct_capit(server=\"localhost:50051\",\n",
    "                model=\"riva_punctuation\",\n",
    "                query=\"how are you doing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-roberts",
   "metadata": {},
   "source": [
    "You can stop all docker container before shutting down the jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $(docker ps -a -q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
